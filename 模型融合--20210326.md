# 心跳信号预测
## 延续前面的特征工程
1）对心跳信号进行拆分处理：在处理的过程中对不同类型的数值型数据进行减少内存的处理，同前面笔记   
2）拆分完信号后得到原始的285个特征   
3）tsfresh处理得到时间序列变化特征，这里注意一个小技巧：在得到训练集和验证集的时间序列特征后先进行合并，共同去除缺失值，这样避免验证集特有的缺失值，但本次比赛经过挖掘发现无变化   
4）用tsfresh对训练集和训练集的标签进行过滤剩余708个特征   
5）将两部分特征进行融合   
## 确定本次比赛的loss
	def abs_sum(y_pre,y_tru):
    y_pre=np.array(y_pre)
    y_tru=np.array(y_tru)
    loss=sum(sum(abs(y_pre-y_tru)))
    return loss
## 融合加权方法
### lightgbm----第一部分
这里的第一部分模型依旧用提供的baseline,未调参  
	
	def cv_model(clf, train_x, train_y, test_x, clf_name):
	    folds = 5
	    seed = 2021
	    kf = KFold(n_splits=folds, shuffle=True, random_state=seed)
	    test = np.zeros((test_x.shape[0],4))
	
	    cv_scores = []
	    onehot_encoder = OneHotEncoder(sparse=False)
	    for i, (train_index, valid_index) in enumerate(kf.split(train_x, train_y)):
	        print('************************************ {} ************************************'.format(str(i+1)))
	        trn_x, trn_y, val_x, val_y = train_x.iloc[train_index], train_y[train_index], train_x.iloc[valid_index], train_y[valid_index]
	        
	        if clf_name == "lgb":
	            train_matrix = clf.Dataset(trn_x, label=trn_y)
	            valid_matrix = clf.Dataset(val_x, label=val_y)
	
	            params = {
	                'boosting_type': 'gbdt',
	                'objective': 'multiclass',
	                'num_class': 4,
	                'num_leaves': 2 ** 5,
	                'feature_fraction': 0.8,
	                'bagging_fraction': 0.8,
	                'bagging_freq': 4,
	                'learning_rate': 0.1,
	                'seed': seed,
	                'nthread': 28,
	                'n_jobs':24,
	                'verbose': -1,
	            }
	
	            model = clf.train(params, 
	                      train_set=train_matrix, 
	                      valid_sets=valid_matrix, 
	                      num_boost_round=2000, 
	                      verbose_eval=100, 
	                      early_stopping_rounds=200)
	            val_pred = model.predict(val_x, num_iteration=model.best_iteration)
	            test_pred = model.predict(test_x, num_iteration=model.best_iteration) 
	            
	        val_y=np.array(val_y).reshape(-1, 1)
	        val_y = onehot_encoder.fit_transform(val_y)
	        print('预测的概率矩阵为：')
	        print(test_pred)
	        test += test_pred
	        score=abs_sum(val_y, val_pred)
	        cv_scores.append(score)
	        print(cv_scores)
	    print("%s_scotrainre_list:" % clf_name, cv_scores)
	    print("%s_score_mean:" % clf_name, np.mean(cv_scores))
	    print("%s_score_std:" % clf_name, np.std(cv_scores))
	    test=test/kf.n_splits
	
	    return test
   
### 随机森林---第一部分---亦无调参  

	def rf_cv_model(x_train, y_train, x_test):
	    folds = 5
	    seed = 2021
	    kf = KFold(n_splits=folds, shuffle=True, random_state=seed)
	    test = np.zeros((x_test.shape[0],4))
	
	    cv_scores = []
	    onehot_encoder = OneHotEncoder(sparse=False)
	    for i, (train_index, valid_index) in enumerate(kf.split(X_train, y_train)):
	        print('************************************ {} ************************************'.format(str(i+1)))
	        trn_x, trn_y, val_x, val_y = X_train.iloc[train_index], y_train[train_index], X_train.iloc[valid_index], y_train[valid_index]
	        model = RandomForestClassifier(n_estimators=100).fit(trn_x,trn_y)
	        test_pred = model.predict(x_test) 
	        val_pred = model.predict(val_x)
	        val_pred=np.array(val_pred).reshape(-1, 1)
	        val_pred = onehot_encoder.fit_transform(val_pred)
	        val_y=np.array(val_y).reshape(-1, 1)
	        val_y = onehot_encoder.fit_transform(val_y)
	        test_pred=np.array(test_pred).reshape(-1, 1)
	        test_pred = onehot_encoder.fit_transform(test_pred)
	        test += test_pred
	        print('预测的概率矩阵为：')
	        score=abs_sum(val_y, val_pred)
	        cv_scores.append(score)
	        print(cv_scores)
	    print("%s_scotrainre_list:" %  cv_scores)
	    print("%s_score_mean:" %  np.mean(cv_scores))
	    print("%s_score_std:" %  np.std(cv_scores))
	    test=test/kf.n_splits
	    
	    return test

### 最后融合模型---加权方式
训练集的验证集部分发现lightGBM更好，因此给更高的权重  
	def Weighted_method(test_pre1,test_pre2,w=[0.6,0.4]):
	    Weighted_result = w[0]*test_pre1+w[1]*test_pre2
	    return Weighted_result
最后提交结果：503